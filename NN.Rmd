---
title: "R Notebook"
output: html_notebook
---

```{r}
library(ggplot2)
library(caret)

```

Creating data frames to hold the data
```{r}
X <- data.frame() # Store the Data matrix
y <- data.frame() # Stores Labels for the classes ( 0 to 9)

```


Creating the NN architecture
```{r}

nnet <- function(X, Y, step_size = 0.5, reg = 0.001, h = 10, niteration){
  N <- nrow(X) # number of observations
  K <- ncol(Y) # number of labels
  D <- ncol(X) # dimensions
  
  
  #Constructing a 2 layer NN
  
  # Random initialization of NN parameters
  W <- 0.01 * matrix(rnorm(D*h), nrow = D)
  b <- matrix(0, nrow = 1, ncol = h)
  W2 <- 0.01 * matrix(rnorm(h*K), nrow = h)
  b2 <- matrix(0, nrow = 1, ncol = K)
  
  # GDA loop to update weight and bias
  for (i in 0:niteration){
    # hidden layer, ReLU activation
    hidden_layer <- pmax(0, X%*% W + matrix(rep(b,N), nrow = N, byrow = T))
    hidden_layer <- matrix(hidden_layer, nrow = N)
    
    # scores for each class
    scores <- hidden_layer%*%W2 + matrix(rep(b2,N), nrow = N, byrow = T)
    
    # computing and normalizing the probabilities of each class
    exp_scores <- exp(scores)
    probs <- exp_scores / rowSums(exp_scores)
    
    # using softmax and regularization to compute the loss
    corect_logprobs <- -log(probs)
    data_loss <- sum(corect_logprobs*Y)/N
    reg_loss <- 0.5*reg*sum(W*W) + 0.5*reg*sum(W2*W2)
    loss <- data_loss + reg_loss
    # process tracker
    if (i%%1000 == 0 | i == niteration){
      print(paste("iteration", i,': loss', loss))}
    
    #Gradient on scores
    dscores <- probs-Y
    dscores <- dscores/N
    
    # BAckpropagating the graident to NN parameters
    dW2 <- t(hidden_layer)%*%dscores
    db2 <- colSums(dscores)
    # Backpropagate to hiddden layers
    dhidden <- dscores%*%t(W2)
    # Backpropagate for ReLU non-linearity
    dhidden[hidden_layer <= 0] <- 0
    # Backpropagate into W,b
    dW <- t(X)%*%dhidden
    db <- colSums(dhidden)
    
    # adjusting the weights
    dW2 <- dW2 + reg *W2
    dW <- dW + reg *W
    
    # model parameter updates
    W <- W-step_size*dW
    b <- b-step_size*db
    W2 <- W2-step_size*dW2
    b2 <- b2-step_size*db2
  }
  return(list(W, b, W2, b2))
}
```

Prediction function

```{r}
nnetPred <- function(X, para = list()){
  W <- para[[1]]
  b <- para[[2]]
  W2 <- para[[3]]
  b2 <- para[[4]]
  
  N <- nrow(X)
  hidden_layer <- pmax(0, X%*% W + matrix(rep(b,N), nrow = N, byrow = T)) 
  hidden_layer <- matrix(hidden_layer, nrow = N)
  scores <- hidden_layer%*%W2 + matrix(rep(b2,N), nrow = N, byrow = T) 
  predicted_class <- apply(scores, 1, which.max)

  return(predicted_class)  
}

```


Helper function to display the digits
```{r}


showDigit <- function(X){
  mat <- matrix(unlist(X),nrow = 28,byrow = T)
  mat <- t(apply(mat, 2, rev))
  image(mat,col=grey.colors(255))
}

train <- read.csv("D:/Work/MachineLearning/Project/Data/train.csv", header = TRUE, stringsAsFactors = F)
showDigit(train[9,-1])

```

Preprocessing the dataset

```{r}
zerovar <- nearZeroVar(train)
zerovar.nolabel <- zerovar-1

splitTrain <- createDataPartition(y=train$label, p=0.8, list=F)

training <- train[splitTrain, ]
test <- train[-splitTrain, ]

X <- as.matrix(training[, -1])
NData <- nrow(X) # total number of obserations
y <- training[, 1] # class variables

C <- length(unique(y)) #number of labels
X.proc <- X[, -zerovar.nolabel]/max(X) # scaling the data
D <- ncol(X.proc) # dimensionality

Xtest <- as.matrix(test[, -1]) # data matrix (each row = single observation)
ytest <- test[, 1] # cross validation class labels
Xtest.proc <- Xtest[, -zerovar.nolabel]/max(X) # scaling the Cross Validation data

Y <- matrix(0, NData, C)

for (i in 1:NData){
  Y[i, y[i]+1] <- 1
}
```


Training the model
```{r}
nnet.model <- nnet(X.proc, Y, step_size = 0.3, reg = 0.0001, niteration = 3000)

```

Finding the training set accuracy
```{r}
predicted_label <- nnetPred(X.proc, nnet.model)
print(paste('training set accuracy:',mean(predicted_label == (y+1))))

```


Finding the testing set accuracy
```{r}
predicted_label <- nnetPred(Xtest.proc, nnet.model)
print(paste('Test accuracy:',mean(predicted_label == (ytest+1))))
```

